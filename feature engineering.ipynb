{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e04037f-9f07-4a41-be03-d3bb28c7d2ee",
   "metadata": {},
   "source": [
    "### Explore feature engineering\n",
    "\n",
    "In this reading, you will learn more about what happens in the Analyze stage of PACE—namely, feature engineering. The meaning of the term “feature engineering” can vary broadly, it includes feature selection, feature transformation, and feature extraction. \n",
    "\n",
    "You will come to understand more about the considerations and process of adjusting your predictor variables to improve model performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9997eec8-4b01-48b7-bc59-ec9c426ea510",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "When building machine learning models, your model is only ever going to be as good as your data. \n",
    "\n",
    "Sometimes, the data you have will not be predictive of your target variable. \n",
    "\n",
    "For example, it’s unlikely that you can build a good model that predicts rainfall if you train it on historical stock market data. \n",
    "\n",
    "In this case, it might seem obvious, but when you’re building a model, you’ll often have features that plausibly could be predictive of your target, but in fact are not. \n",
    "\n",
    "Other times, your model’s features might contain a predictive signal for your model, but this signal can be strengthened if you manipulate the feature in a way that makes it more detectable by the model.\n",
    "\n",
    "Feature engineering is the process of using practical, statistical, and data science knowledge to select, transform, or extract characteristics, properties, and attributes from raw data. In this reading, you will learn more about these processes, when and why to use them, and what good feature engineering can do for your model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be38ea3-25e9-4a12-8962-b6718103c08d",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "Feature selection is the process of picking variables from a dataset that will be used as predictor variables for your model. \n",
    "\n",
    "With very large datasets, there are dozens if not hundreds of features for each observation in the data. \n",
    "\n",
    "Using all of the features in a dataset often doesn’t give any performance boost. \n",
    "\n",
    "In fact, it may actually hurt performance by adding complexity and noise to the model. \n",
    "\n",
    "Therefore, choosing the features to use for the model is an important part of the model development process. \n",
    "\n",
    "Generally, there are three types of features:\n",
    "\n",
    "Predictive: Features that by themselves contain information useful to predict the target                       \n",
    "\n",
    "Interactive: Features that are not useful by themselves to predict the target variable, but become predictive in conjunction with other features\n",
    "\n",
    "Irrelevant: Features that don’t contain any useful information to predict the target\n",
    "\n",
    "You want predictive features, but a predictive feature can also be a redundant feature. \n",
    "\n",
    "Redundant features are highly correlated with other features and therefore do not provide the model with any new information—for example, the steps you took in a day, may be highly correlated with the calories you burned. \n",
    "\n",
    "The goal of feature selection is to find the predictive and interactive features and exclude redundant and irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8c99c-4e74-435f-b050-be9415e1963f",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f4afe0-e557-4b9c-b127-e632873bbaf1",
   "metadata": {},
   "source": [
    "##### The feature selection process typically occurs at multiple stages of the PACE workflow. \n",
    "\n",
    "##### The first place it occurs is during the Plan phase. \n",
    "\n",
    "Once you have defined your problem and decided on a target variable to predict, you need to find features. \n",
    "\n",
    "Keep in mind that datasets are not always prepackaged in nice little tables ready to model. \n",
    "\n",
    "Data professionals can spend days, weeks, or even months acquiring and assembling features from many different sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62dab3e-f3fb-4ec7-a349-0592c94b2581",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aed8d2-80ea-4619-ad33-fb185244de60",
   "metadata": {},
   "source": [
    "##### Feature selection can happen once more during the Analyze phase. \n",
    "\n",
    "Once you do an exploratory data analysis, it might become clear that some of the features you included are not suitable for modeling. \n",
    "\n",
    "This could be for a number of reasons. \n",
    "\n",
    "For example, you might find that a feature has too many missing or clearly erroneous values, or perhaps it’s highly correlated with another feature and must be dropped so as not to violate the assumptions of your model. \n",
    "\n",
    "It’s also common that the feature is some kind of metadata, such as an ID number with no inherent meaning. Whatever the case may be, you might want to drop these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7997aa70-ffc2-43bf-9c51-047dde7bfbe8",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f53b0-7a80-4d24-9532-44949177b887",
   "metadata": {},
   "source": [
    "##### During the Construct phase, when you are building models, \n",
    "\n",
    "the process of improving your model might include more feature selection. \n",
    "\n",
    "At this point, the objective usually is to find the smallest set of predictive features that still results in good overall model performance. \n",
    "\n",
    "In fact, data professionals will often base final model selection not solely on score, but also on model simplicity and explainability. \n",
    "\n",
    "A model with an R2 of 0.92 and 10 features might get selected over a model with an R2 of 0.94 and 60 features. \n",
    "\n",
    "Models with fewer features are simpler, and simpler models are generally more stable and easier to understand.\n",
    "\n",
    "When data professionals perform feature selection during the Construct phase, they typically use statistical methodologies to determine which features to keep and which to drop. \n",
    "\n",
    "It could be as simple as ranking the model’s feature importances and keeping only the top a% of them. \n",
    "\n",
    "Another way of doing it is to keep the top features that account for ≥ b% of the model’s predictive signal. \n",
    "\n",
    "There are many different ways of performing feature selection, but they all seek to keep the predictive features and exclude the non-predictive features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbbe5b-27eb-41db-beb6-c7101e1db67d",
   "metadata": {},
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9fac83-d14f-4e7d-b2cd-769b8eafd665",
   "metadata": {},
   "source": [
    "#### Feature Transformation\n",
    "Feature transformation is a process where you take features that already exist in the dataset, and alter them so that they’re better suited to be used for training the model. \n",
    "\n",
    "Data professionals usually perform feature transformation during the Construct phase, after they’ve analyzed the data and made decisions about how to transform it based on what they’ve learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918dd2ca-ddea-4040-9872-59d1202d6966",
   "metadata": {},
   "source": [
    "##### Log normalization\n",
    "There are various types of transformations that might be required for any given model. \n",
    "\n",
    "\n",
    "For example, some models do not handle continuous variables with skewed distributions very well. \n",
    "\n",
    "As a solution, you can take the log of a skewed feature, reducing the skew and making the data better for modeling. This is known as log normalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab66567-e70a-417e-85c4-163175b40dac",
   "metadata": {},
   "source": [
    "For instance, suppose you had a feature X1 whose histogram demonstrated the following distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "175f2fe3-0b30-4798-b5c4-53dc8ea5c6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/0f2PLzdySI-yBh1f4zL_zQ_b762f5dd46634700b3fbbc425b4ff0f1_ADA_R-143_lognormal-distribution.png?expiry=1729209600000&hmac=hTQJKrn1repkBH4hA9rUceqNc_Qjrll4PDzeKvs6YVY\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(url='https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/0f2PLzdySI-yBh1f4zL_zQ_b762f5dd46634700b3fbbc425b4ff0f1_ADA_R-143_lognormal-distribution.png?expiry=1729209600000&hmac=hTQJKrn1repkBH4hA9rUceqNc_Qjrll4PDzeKvs6YVY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9897c3-8b87-4006-a580-c3dc39fb01f8",
   "metadata": {},
   "source": [
    "This is known as a log-normal distribution. \n",
    "\n",
    "A log-normal distribution is a continuous distribution whose logarithm is normally distributed. \n",
    "\n",
    "In this case, the distribution skews right, but if you transform the feature by taking its natural log, it normalizes the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a071fdee-135a-45c3-b610-a5533fb61cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/4x4ZbeD2R7OFKtimqMf9HQ_ea346ad8550e4a46ad9d0b096c0a3ff1_ADA_R-143_normal-distribution.png?expiry=1729209600000&hmac=s7_3x5sh0UDUcgLV5mESl_EL7ObIZ901qDfNEh2Cw9U\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(url='https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/4x4ZbeD2R7OFKtimqMf9HQ_ea346ad8550e4a46ad9d0b096c0a3ff1_ADA_R-143_normal-distribution.png?expiry=1729209600000&hmac=s7_3x5sh0UDUcgLV5mESl_EL7ObIZ901qDfNEh2Cw9U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cb6a46-ee42-4e4f-a03d-6499f79832a0",
   "metadata": {},
   "source": [
    "Normalizing a feature’s distribution is often better for training a model, and you can later verify whether or not taking the log has helped by analyzing the model’s performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c591b4c2-0f2a-42b0-9c03-77be5c6a7462",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd38e4f5-cfdd-4074-b3f2-eba37d60314b",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "Another kind of feature transformation is scaling. \n",
    "\n",
    "Scaling is when you adjust the range of a feature’s values by applying a normalization function to them. \n",
    "\n",
    "Scaling helps prevent features with very large values from having undue influence over a model compared to features with smaller values, but which may be equally important as predictors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3f637-bc80-4649-9005-034e0c8822ee",
   "metadata": {},
   "source": [
    "There are many scaling methodologies available. Some of the most common include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3235582-fc50-4e4a-b2bf-f059a64c53a0",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "Normalization (e.g., MinMaxScaler in scikit-learn) transforms data to reassign each value to fall within the range [0, 1]. \n",
    "\n",
    "When applied to a feature, the feature’s minimum value becomes zero and its maximum value becomes one. All other values scale to somewhere between them. The formula for this transformation is:\n",
    "\n",
    "$$x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}$$\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4286b-6474-4db6-990d-f1d515ba1254",
   "metadata": {},
   "source": [
    "For example, suppose you have feature 1, whose values range from 36 to 209; and feature 2, whose values range from 72 to 978:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6555942c-4c61-4274-8f59-84e2f5af5d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/ZO9a8dZMRWC8Nz7ph0gLzA_df62827fc26c4c9d80a56a18b80623f1_ADA_R-143_Scatterplot-of-2-unscaled-features.png?expiry=1729209600000&hmac=4U86CYD5SjvTpRwVQayNudZE5wU8jlExRt2q_-NniQY\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(url='https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/ZO9a8dZMRWC8Nz7ph0gLzA_df62827fc26c4c9d80a56a18b80623f1_ADA_R-143_Scatterplot-of-2-unscaled-features.png?expiry=1729209600000&hmac=4U86CYD5SjvTpRwVQayNudZE5wU8jlExRt2q_-NniQY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f7c68-0687-4354-877b-6e8bfac1beed",
   "metadata": {},
   "source": [
    "It is apparent that these features are on different scales from one another. \n",
    "\n",
    "Features with higher magnitudes of scale will be more influential in some machine learning algorithms, like K-means, where  Euclidean distances between data points are calculated with the absolute value of the features (so large feature values have major effects, compared to small feature values). \n",
    "\n",
    "By min-max scaling (normalizing) each feature, they are both reduced to the same range:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dbcb909-7879-4ca4-bfe2-dbe34accfd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/FxjouY0iT3eab20GiXCoxQ_785a6047d66342169c2a5104e037b6f1_ADA_R-143_Scatterplot-of-2-normalized-features.png?expiry=1729209600000&hmac=rlRefCQWt0UYpZSgx8JT8MbrA4EoAMAtf0NxlfYNgE8\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(url='https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/FxjouY0iT3eab20GiXCoxQ_785a6047d66342169c2a5104e037b6f1_ADA_R-143_Scatterplot-of-2-normalized-features.png?expiry=1729209600000&hmac=rlRefCQWt0UYpZSgx8JT8MbrA4EoAMAtf0NxlfYNgE8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6477f0d-a0f6-44e9-83d1-cea20750d266",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cbb9c3-3db1-41a5-88a0-b2238d09db5b",
   "metadata": {},
   "source": [
    "Standardization\n",
    "\n",
    "Another type of scaling is called standardization (e.g., StandardScaler in scikit-learn). \n",
    "\n",
    "Standardization transforms each value within a feature so they collectively have a mean of zero and a standard deviation of one. \n",
    "\n",
    "To do this, for each value, subtract the mean of the feature and divide by the feature’s standard deviation:\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "This method is useful because it centers the feature’s values on zero, which is useful for some machine learning algorithms. \n",
    "\n",
    "It also preserves outliers, since it does not place a hard cap on the range of possible values. \n",
    "\n",
    "Here is the same data from above after applying standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b36c36b3-4097-40ca-9b19-bee1b42ea15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/N3NCgty8SQCMK_BgIBGMbQ_4bba7ed489e949daa4d299925a9a58f1_ADA_R-143_Scatterplot-of-2-standardized-features.png?expiry=1729209600000&hmac=oI39H_lTARwbnUtctRqpeLDc0zX23MF3Nt4CyvirtYc\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(url='https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/N3NCgty8SQCMK_BgIBGMbQ_4bba7ed489e949daa4d299925a9a58f1_ADA_R-143_Scatterplot-of-2-standardized-features.png?expiry=1729209600000&hmac=oI39H_lTARwbnUtctRqpeLDc0zX23MF3Nt4CyvirtYc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5750c29-8d4a-462a-af3d-e34b7e692988",
   "metadata": {},
   "source": [
    "Notice that the points are spatially distributed in a way that is very similar to the result of normalizing, but the values and scales are different. In this case, the values now range from -1.49 to 1.76."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce58ff5-85fd-4bcb-b634-3e892519b6c5",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a987c-c673-4713-baf1-606978faa3d9",
   "metadata": {},
   "source": [
    "#### Encoding\n",
    "Another form of feature transformation is known as encoding. \n",
    "\n",
    "Variable encoding is the process of converting categorical data to numerical data. \n",
    "\n",
    "Consider the bank churn dataset. The original data has a feature called “Geography”, whose values represent each customer’s country of residence—France, Germany, or Spain. \n",
    "\n",
    "Most machine learning methodologies cannot extract meaning from strings. Encoding transforms the strings to numbers that can be interpreted mathematically.\n",
    "\n",
    "The “Geography” column contains nominal values, or values that don’t have an inherent order or ranking. \n",
    "\n",
    "As such, the feature would typically be encoded into binary. This process requires that a column be added to represent each possible class contained within the feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65f5816-5db5-4d76-bcfb-23c3836c44e3",
   "metadata": {},
   "source": [
    "| Geography | France | Germany | Spain |\n",
    "|-----------|--------|---------|-------|\n",
    "| France    | 1      | 0       | 0     |\n",
    "| Germany   | 0      | 1       | 0     |\n",
    "| Spain     | 0      | 0       | 1     |\n",
    "| France    | 1      | 0       | 0     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa856a-7be5-45b8-94a1-85c724bb0e89",
   "metadata": {},
   "source": [
    "Tools commonly used to do this include pandas.get_dummies() and OneHotEncoder().  \n",
    "\n",
    "Often methods drop one of the columns to avoid having redundant information in the dataset . \n",
    "\n",
    "Note that information isn’t lost by doing this. If you have this…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc08e37-5788-4e92-815b-9c59a696e37d",
   "metadata": {},
   "source": [
    "| Customer      | France | Germany |\r\n",
    "|---------------|--------|---------|\r\n",
    "| Antonio García| 0      | 0       |\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee6a2e6-942b-48ad-843a-16b1c5c16364",
   "metadata": {},
   "source": [
    "… then you know this customer is from Spain!\n",
    "\n",
    "Keep in mind that some features may be inferred to be numerical by Python or other frameworks but still represent a category. \n",
    "\n",
    "For example, suppose you had a dataset with people assigned to different arbitrary groups: 1, 2, and 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80053a1-1ddf-4119-ae50-87d21f0ba2da",
   "metadata": {},
   "source": [
    "| Name          | Group |\n",
    "|---------------|-------|\n",
    "| Rachel Stein  | 2     |\n",
    "| Ahmed Abadi   | 2     |\n",
    "| Sid Avery     | 3     |\n",
    "| Ha-rin Choi   | 1     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9212b03d-290c-4a08-bf81-781593ffd10f",
   "metadata": {},
   "source": [
    "The “Group” column might be encoded as type int, but the number is really only representative of a category. \n",
    "\n",
    "Group 3 isn’t two units “greater than” group 1. \n",
    "\n",
    "The groups could just as easily be labeled with colors. In this case, you could first convert the column to a string, and then encode the strings as binary columns. \n",
    "\n",
    "This is a problem that can be solved upstream at the stage of data generation: categorical features (like a group) should not be recorded using a number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333b002-bb88-48a1-b289-a314c42c156b",
   "metadata": {},
   "source": [
    "A different kind of encoding can be used for features that contain discrete or ordinal values. This is called ordinal encoding. \n",
    "\n",
    "It is used when the values do contain inherent order or ranking. For instance, consider a “Temperature” column that has values of cold, warm, and hot.\n",
    "In this case, ordinal encoding could reassign these classes to 0, 1, and 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013cc8d3-22a8-41de-93be-7782e959cb8f",
   "metadata": {},
   "source": [
    "| Temperature | Temperature (Ordinal encoding) |\n",
    "|-------------|-------------------------------|\n",
    "| cold        | 0                             |\n",
    "| warm        | 1                             |\n",
    "| hot         | 2                             |\n",
    "\n",
    "This method retains the order or ranking of the classes relative to one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa071061-2a0a-47f8-9b40-09ac4688f87c",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba66b77-faee-4558-a170-8d1c5a48b04a",
   "metadata": {},
   "source": [
    "#### Feature extraction\n",
    "Feature extraction involves producing new features from existing ones, with the goal of having features that deliver more predictive power to your model. \n",
    "\n",
    "While there is some overlap between extraction and transformation colloquially, the main difference is that a new feature is created from one or more other features rather than simply changing one that already exists.\n",
    "\n",
    "Consider a feature called “Date of Last Purchase,” which contains information about when a customer last purchased something from the company. \n",
    "\n",
    "Instead of giving the model raw dates, a new feature can be extracted called “Days Since Last Purchase.” \n",
    "\n",
    "This could tell the model how long it has been since a customer has bought something from the company, giving insight into the likelihood that they’ll buy something again in the future. \n",
    "\n",
    "Suppose that today’s date is May 30th, extracting a new feature could look something like this:\n",
    "\n",
    "| Date of Last Purchase | Days Since Last Purchase |\r\n",
    "|-----------------------|--------------------------|\r\n",
    "| May 17th              | 13                       |\r\n",
    "| May 29th              | 1                        |\r\n",
    "| May 10th              | 20                       |\r\n",
    "| May 21st              | 9                        |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e1301e-1aac-4fb7-ae74-2057a09dc6e2",
   "metadata": {},
   "source": [
    "Features can also be extracted from multiple variables. For example, consider modeling if a customer will return to buy something else. \n",
    "\n",
    "In the data, there are two variables: “Days Since Last Purchase” and “Price of Last Purchase.” \n",
    "\n",
    "A new variable could be created from these by dividing the price by the number of days since the last purchase, creating a new variable altogether.\n",
    "\n",
    "| Days Since Last Purchase | Price of Last Purchase | Dollars Per Day Since Last Purchase |\r\n",
    "|--------------------------|------------------------|-------------------------------------|\r\n",
    "| 13                       | \\$85                   | \\$6.54                             |\r\n",
    "| 1                        | \\$15                   | \\$15.00                            |\r\n",
    "| 20                       | \\$8                    | \\$0.40                             |\r\n",
    "| 9                        | \\$43                   | \\$4.78                          \n",
    "\n",
    " \n",
    "Sometimes, the features that you are able to generate through extraction can offer the greatest performance boosts to your model. \n",
    "\n",
    "It can be a trial and error process, but finding good features from the raw data is what will make a model stand out in industry.   |\r\n",
    "    |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22373cd0-fe6c-421e-81ea-0a124d3c9f6f",
   "metadata": {},
   "source": [
    "#### Key takeaways\n",
    "- Analyzing the features in a dataset is essential to creating a model that will produce valuable results.\n",
    "- **Feature Selection** is the process of dropping any and all unnecessary or unwanted features from the dataset.\n",
    "- **Feature Transformation** is the process of editing features into a form where they’re better for training the model.\n",
    "- **Feature Extraction** is the process of creating brand new features from other features that already exist in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182c478-f875-4dbb-9543-0667a2e77beb",
   "metadata": {},
   "source": [
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab23c7a9-8b5f-403d-b2c8-a577cf7620df",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada45277-2c1c-41b0-9198-427677ddd392",
   "metadata": {},
   "source": [
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
